{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패키지 설치 및 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install youtube_transcript_api\n",
    "!pip install transformers\n",
    "# Run in terminal or command prompt\n",
    "!python -m spacy download en\n",
    "!git clone https://github.com/lovit/textrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcripts, NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from urllib import parse\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "##transcripts_load\n",
    "def transcripts_load(url):\n",
    "        url_data = parse.urlparse(url)\n",
    "        query = parse.parse_qs(url_data.query)\n",
    "        video = query[\"v\"][0]\n",
    "        transcripts = YouTubeTranscriptApi.get_transcript(video)\n",
    "        return transcripts\n",
    "\n",
    "##transcripts split by seperator\n",
    "def transcripts_split(transcripts, seperator):\n",
    "    new_transcripts = []\n",
    "    for transcript in transcripts:\n",
    "        duration = 0\n",
    "        if (transcript['text'][-1]!=seperator)&(seperator in transcript['text']):\n",
    "            for text in transcript['text'].split(seperator):\n",
    "                if transcript['text'].split(seperator).index(text)!=len(transcript['text'].split(seperator))-1:\n",
    "                    new_transcript = {'text':text + seperator, 'start':transcript['start']+duration, \n",
    "                                    'duration':transcript['duration']/len(transcript['text'].split(seperator))}\n",
    "                    new_transcripts.append(new_transcript)\n",
    "                    duration = transcript['duration']/len(transcript['text'].split(seperator))\n",
    "                else:\n",
    "                    new_transcript = {'text':text, 'start':transcript['start']+duration, \n",
    "                                    'duration':transcript['duration']/len(transcript['text'].split(seperator))}            \n",
    "                    new_transcripts.append(new_transcript)\n",
    "        else:\n",
    "            new_transcripts.append(transcript)            \n",
    "    return new_transcripts\n",
    "    \n",
    "##transcripts sum\n",
    "def transcripts_sum(transcripts):\n",
    "    # 문장단위로 transcript 합치기\n",
    "    new_transcripts = []\n",
    "    temp_text = ''\n",
    "    temp_start = 0\n",
    "    temp_duration = 0\n",
    "\n",
    "    for transcript in transcripts:\n",
    "        if ('.'==transcript['text'][-1])|('?'==transcript['text'][-1])|('!'==transcript['text'][-1]):\n",
    "            if temp_text:\n",
    "                temp_text = temp_text + transcript['text'] + ' '\n",
    "                temp_duration += transcript['duration']\n",
    "                new_transcript = {'text':temp_text, 'start':temp_start, 'duration':temp_duration}\n",
    "\n",
    "                new_transcripts.append(new_transcript)\n",
    "\n",
    "                temp_text = ''\n",
    "                temp_start = 0\n",
    "                temp_duration = 0\n",
    "            else:\n",
    "                new_transcripts.append(transcript)\n",
    "                temp_text = ''\n",
    "                temp_start = 0\n",
    "                temp_duration = 0\n",
    "        else:\n",
    "            temp_text = temp_text + transcript['text'] + ' '\n",
    "            temp_duration += transcript['duration']\n",
    "            temp_start = transcript['start']\n",
    "            \n",
    "    return new_transcripts\n",
    "\n",
    "##transcripts remove stopwords\n",
    "def transcripts_remove_stopwords(transcripts, stopwords):\n",
    "    for transcript in transcripts:\n",
    "        transcript['text'] = transcript['text'].lower()\n",
    "        transcript['text'] = re.sub(r\"\\[([A-Za-z0-9_]+)\\] \", '', transcript['text']).strip()\n",
    "        transcript['text'] = re.sub(r\"\\[([A-Za-z0-9_]+)\\]\", '', transcript['text']).strip()\n",
    "        transcript['text'] = transcript['text'].replace('\\n', ' ').strip()\n",
    "        for word in stopwords:\n",
    "            transcript['text'] = transcript['text'].replace(word, '').strip()\n",
    "            \n",
    "    return transcripts\n",
    "\n",
    "    \n",
    "class Transcripts:\n",
    "    def __init__(self, url, stopwords=['um, ', 'um,', 'um', 'uh, ', 'uh,', 'uh', 'you know, ', 'you know,']):\n",
    "        self.transcripts = transcripts_load(url)\n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "        self.bertTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bertNSP = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "        self.losses = None\n",
    "        self.threshold = None\n",
    "        self.index = [-1]\n",
    "        \n",
    "    def transcripts_preprocess(self):\n",
    "        # 문장단위로 transcript 나누기            \n",
    "        new_transcripts = transcripts_split(self.transcripts, '.')\n",
    "        new_transcripts = transcripts_split(new_transcripts, '?')\n",
    "        new_transcripts = transcripts_split(new_transcripts, '!')\n",
    "                \n",
    "        # 문장단위로 transcript 합치기\n",
    "        new_transcripts = transcripts_sum(new_transcripts)\n",
    "        \n",
    "        # 문장 내 의미없는 감탄사 등 지우기\n",
    "        new_transcripts = transcripts_remove_stopwords(new_transcripts, self.stopwords)\n",
    "        self.transcripts = new_transcripts\n",
    "    \n",
    "    def make_losses(self):\n",
    "        self.losses = []\n",
    "\n",
    "        for i in range(len(self.transcripts)-1):\n",
    "            prompt = self.transcripts[i]['text']\n",
    "            next_sentence = self.transcripts[i+1]['text']\n",
    "            encoding = self.bertTokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "            loss, logits = self.bertNSP(**encoding, next_sentence_label=torch.LongTensor([1]))\n",
    "\n",
    "            self.losses.append(float(loss))\n",
    "            \n",
    "    def make_threshold(self, percent=2):\n",
    "        self.threshold = np.percentile(self.losses, percent)\n",
    "        \n",
    "    def transcripts_intergrate(self):\n",
    "        for i in range(len(self.losses)):\n",
    "            if self.losses[i] < self.threshold:\n",
    "                self.index.append(i)\n",
    "        \n",
    "        transcripts_intergrated = []\n",
    "        temp_text = \"\"\n",
    "        temp_start = 0\n",
    "        temp_duration = 0\n",
    "        for i in range(len(self.transcripts)):\n",
    "            if i-1 in self.index:\n",
    "                if temp_text:\n",
    "                    new_transcript = {'text':temp_text, 'start':temp_start, 'duration':temp_duration}\n",
    "                    transcripts_intergrated.append(new_transcript)\n",
    "                \n",
    "                temp_text = \"\"\n",
    "                temp_start = 0\n",
    "                temp_duration = 0\n",
    "                    \n",
    "                temp_text += self.transcripts[i]['text']\n",
    "                temp_start = self.transcripts[i]['start']\n",
    "                temp_duration += self.transcripts[i]['duration']\n",
    "            \n",
    "            elif i==(len(self.transcripts)-1):\n",
    "                temp_text += self.transcripts[i]['text']\n",
    "                temp_duration += self.transcripts[i]['duration']\n",
    "                new_transcript = {'text':temp_text, 'start':temp_start, 'duration':temp_duration}\n",
    "                transcripts_intergrated.append(new_transcript)\n",
    "            \n",
    "            else:\n",
    "                temp_text += self.transcripts[i]['text']\n",
    "                temp_duration += self.transcripts[i]['duration']\n",
    "        self.transcripts = transcripts_intergrated\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.youtube.com/watch?v=bNb2fEVKeEo&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=5&ab_channel=StanfordUniversitySchoolofEngineering\"\n",
    "transcripts = Transcripts(url)\n",
    "transcripts.transcripts_preprocess()\n",
    "transcripts.make_losses()\n",
    "transcripts.make_threshold()\n",
    "transcripts.transcripts_intergrate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeySentence Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yscho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, spacy\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "nltk.download('stopwords')\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manysents_to_words(sentences):\n",
    "        return [simple_preprocess(str(sentence), deacc=True) for sentence in sentences]\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, transcripts):\n",
    "        self.paragraphs = [transcript['text'] for transcript in transcripts]\n",
    "        self.start = [transcript['start'] for transcript in transcripts]\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.words = manysents_to_words(self.paragraphs)\n",
    "        \n",
    "        # Build the bigram and trigram models\n",
    "        self.bigram = gensim.models.Phrases(self.words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        self.trigram = gensim.models.Phrases(self.bigram[self.words], threshold=100)\n",
    "\n",
    "        # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "        self.bigram_mod = gensim.models.phrases.Phraser(self.bigram)\n",
    "        self.trigram_mod = gensim.models.phrases.Phraser(self.trigram)\n",
    "\n",
    "        self.nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "        self.sentences = None\n",
    "    \n",
    "    def sentence_tokenizer(self, sentence, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        #onesent_to_words\n",
    "        tokens = simple_preprocess(sentence, deacc=True)\n",
    "\n",
    "        #no_stopwords\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "\n",
    "        #make_bigrams\n",
    "        tokens = self.bigram_mod[tokens]\n",
    "\n",
    "        #lemmatization\n",
    "        doc = self.nlp(\" \".join(tokens))\n",
    "        return [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "    \n",
    "    def make_sentences(self):\n",
    "        self.sentences = []\n",
    "        for paragraph in self.paragraphs:\n",
    "            temp = [sentence.strip() for sentence in  paragraph.split('.') if sentence.strip()]\n",
    "            self.sentences.append(temp)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(transcripts.transcripts)\n",
    "tokenizer.make_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeySentence Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textrank.textrank.summarizer import KeysentenceSummarizer\n",
    "from textrank.textrank.summarizer import KeywordSummarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoChaptercreatoR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:08 okay, so what we just saw earlier, this is taking one filter, sliding it over all of the spatial locations in the image and then we're going to get this activation map out, right, which is the value of that filter at every spatial location \n",
      "\n",
      "0:25:39 right, and so just kind of looking at the way that we computed how many, what the output size is going to be, this actually can work into a nice formula where we take our dimension of our input n, we have our filter size f, we have our stride at which we're sliding along, and our final output size, the spatial dimension of each output size is going to be n minus f divided by the stride plus one, right, and you can kind of see this as a, if i'm going to take my filter, let's say i fill it in at the very last possible position that it can be in and then take all the pixels before that, how many instances of moving by this stride can i fit in \n",
      "\n",
      "0:36:49 so remember each filter is going to do a dot product through the entire depth of your input vole \n",
      "\n",
      "0:38:46 [muffled speaking] yeah, so the question is, does the zero padding add some sort of extraneous features at the corners?and yeah, so i mean, we're doing our best to still, get some value and do, like, process that region of the image, and so zero padding is kind of one way to do this, where i guess we can, we are detecting part of this template in this region \n",
      "\n",
      "0:39:36 okay, so, yeah, so we saw how padding can basically help you maintain the size of the output that you want, as well as apply your filter at these, like, corner regions and edge regions \n",
      "\n",
      "0:44:19 can you guys speak up?250, okay so i heard 250, which is close, but remember that we're also, our input vole, each of these filters goes through by depth \n",
      "\n",
      "0:44:50 and so at one sense it's kind of the resolution at which you slide it on, and usually the reason behind this is because when we have a larger stride what we end up getting as the output is a down sampled image, right, and so what this downsampled image lets us have is both, it's a way, it's kind of like pooling in a sense but it's just a different and sometimes works better way of doing pooling is one of the intuitions behind this, 'cause you get the same effect of downsampling your image, and then also as you're doing this you're reducing the size of the activation maps that you're dealing with at each layer, right, and so this also affects later on the total nber of parameters that you have because for example at the end of all your conv layers, now you might put on fully connected layers on top, for example, and now the fully connected layer's going to be connected to every value of your convolutional output, right, and so a smaller one will give you smaller nber of parameters, and so now you can get into, like, basically thinking about trade offs of, nber of parameters you have, the size of your model, overfitting, things like that, and so yeah, these are kind of some of the things that you want to think about with choosing your stride \n",
      "\n",
      "0:53:12 so in this case our pooling layer also has a filter size and this filter size is going to be the region at which we pool over, right, so in this case if we have two by two filters, we're going to slide this, and so, here, we also have stride two in this case, so we're going to take this filter and we're going to slide it along our input vole in exactly the same way as we did for convolution \n",
      "\n",
      "0:56:23 [muffled speaking] yeah, so the question is, is it typical to set up the stride so that there isn't an overlap?and yeah, so for the pooling layers it is, i think the more common thing to do is to have them not have any overlap, and i guess the way you can think about this is basically we just want to downsample and so it makes sense to kind of look at this region and just get one value to represent this region and then just look at the next region and so on \n",
      "\n",
      "0:56:55 okay, so yeah, so with these pooling layers, so again, there's right, some design choices that you make, you take this input vole of w by h by d, and then you're going to set your hyperparameters for design choices of your filter size or the spatial extent over which you are pooling, as well as your stride, and then you can again compute your output vole using the same equation that you used earlier for convolution, it still applies here, right, so we still have our w total extent minus filter size divided by stride plus one \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "summarizer = KeysentenceSummarizer(\n",
    "    tokenize = tokenizer.sentence_tokenizer,\n",
    "    min_sim = 0.3,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "for i in range(len(tokenizer.sentences)):\n",
    "    #if len(tokenizer.sentences[i])>5:\n",
    "    keysents = summarizer.summarize(tokenizer.sentences[i], topk=1)\n",
    "    start = tokenizer.start[i]\n",
    "    print(str(datetime.timedelta(seconds=int(start))), end=' ')\n",
    "    for _, _, sent in keysents:\n",
    "        print(sent, end=' ')\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
